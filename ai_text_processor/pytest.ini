[tool.pytest.ini_options]
minversion = "6.0"
addopts = [
    "-ra",
    "--strict-markers",
    "--strict-config",
    "--cov=src",
    "--cov-report=term-missing:skip-covered",
    "--cov-report=html",
    "--cov-report=xml",
    "--cov-fail-under=80",
    "--benchmark-only",
    "--benchmark-sort=mean",
    "--benchmark-columns=min,max,mean,stddev,rounds,iterations",
    "--benchmark-json=benchmark.json"
]
testpaths = ["tests"]
python_files = ["test_*.py", "*_test.py", "tests.py"]
python_classes = ["Test*", "*Tests"]
python_functions = ["test_*"]
markers = [
    "unit: marks tests as unit tests",
    "integration: marks tests as integration tests",
    "performance: marks tests as performance tests",
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "gpu: marks tests that require GPU",
    "network: marks tests that require network access"
]
filterwarnings = [
    "error",
    "ignore::UserWarning",
    "ignore::DeprecationWarning",
    "ignore::PendingDeprecationWarning",
]
log_cli = true
log_cli_level = "INFO"
log_cli_format = "%(asctime)s [%(levelname)8s] %(name)s: %(message)s"
log_cli_date_format = "%Y-%m-%d %H:%M:%S"
asyncio_mode = "auto"

[coverage:run]
source = ["src"]
omit = [
    "*/tests/*",
    "*/test_*.py",
    "*/__pycache__/*",
    "*/venv/*",
    "*/env/*",
    "setup.py",
    "conftest.py"
]
branch = true
parallel = true

[coverage:report]
precision = 2
show_missing = true
skip_covered = false
sort = "Cover"
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "raise AssertionError",
    "raise NotImplementedError",
    "if __name__ == .__main__.:",
    "if TYPE_CHECKING:",
]

[coverage:html]
directory = "htmlcov"

[coverage:xml]
output = "coverage.xml"

[coverage:json]
output = "coverage.json"

[benchmarks]
ignore_errors = true
enable = true
sort = "mean"
columns = "min,max,mean,stddev,rounds,iterations"
group-by = "group"
legacy-json = false
json-format = "3"